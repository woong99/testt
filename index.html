<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title></title>
    <link rel="stylesheet" href="./index.css" />
  </head>
  <body>
    <h1>소켓 프로그래밍: 네트워크 애플리케이션 생성</h1>
    <p>
      클라이언트-서버 구조 애플리케이션에는 두 가지 형태가 있음. -표준 프로토콜을 구현하는
      클라이언트-서버 애플리케이션 --개방형이라고도 하며 RFC에 정의된 규칙을 따른다. 이때 클라이언트
      와 서버 개발자가 따로 있더라도 RFC 규칙을 철저하게 따른다면 문제 없이 통신한다. 오늘날
      대부분의 네트워크 애플리케이션은 독립 개발자 가 개발한 클라이언트와 서버 프로그램 간의 통신을
      포함한다. -개인의 독점적인 네트워크 애플리케이션 --RFC나 다른 곳에 공식적으로 출판되지 않은
      애플리케이션 계층 프로 토콜을 채택하며 개발자가 클라이언트와 서버 프로그램 모두를 생성하고
      코드에 무엇을 사용할지 완전히 제어하기 때문에 다른 독립 개발자는 이 애플리케이션과
      상호작용하는 코드를 개발할 수 없다.
    </p>
    <h1>UDP를 이용한 소켓 프로그래밍</h1>
    <p>
      애플리케이션 개발자는 소켓 애플리케이션 계층에 대한 제어권은 가지 고 있으나 트랜스포트 계층의
      제어권은 거의 없다. UDP 통신의 경우 프로세스가 데이터의 패킷을 소켓 문밖으로 밀어내 기 전에
      패킷에 목적지 주소를 붙이는데 이 주소에는 호스트의 IP 주소 가 목적지 주소의 일부가 된다. 이때
      호스트는 여러 소켓을 갖는 네트워 크 애플리케이션을 수행할 수가 있으므로 소켓이 생성될 때 포트
      번호라 고 하는 식별자가 소켓에 할당되고 이는 목적지 주소에도 포함이 된다.
    </p>
    <h1>TCP 소켓 프로그래밍</h1>
    <p>
      TCP 연결은 연결지향 프로토콜로 데이터 송신 전에 TCP 연결을 설정할 필요가 있으며 연결 시
      클라이언트와 서버의 소켓 주소(IP주소와 포트번 호)를 연결과 연관시킨다. TCP에서 클라이언트와
      서버의 상호작용 시 서버는 클라이언트가 접속하기 전에 프로세스를 수행하고 있어야 하며
      클라이언트의 초기 접속을 처리하는 특별한 출입문을 가져야 한다. 클라 이언트-서버 연결을
      시도하기 위해서는 클라이언트 프로그램에서 소켓을 생성하는 것이 필요하며 이때 서버의 환영 소켓
      주소를 명시한다. 이후 핸드셰이킹 과정이 일어나는데 이를 클라이언트와 서버 프로그램은 인식 하지
      못한다. 핸드셰이킹 과정에서 서버는 해당 클라이언트에게 지정되 는 새로운 소켓을 생성한다. 이때
      생성되는 소켓은 초기 환영 소켓과는 다른 소켓이다.
    </p>
    <h1>트랜스포트 계층 서비스 및 개요</h1>
    <p>
      트랜스포트 계층 프로토콜은 애플리케이션 프로세스 간의 논리적 통신을 제공하는데 이는
      애플리케이션 관점에서는 프로세스들이 동작하는 호스 트들이 직접 연결된 것처럼 보인다는 것을
      의미한다. 트랜스포트 계층 프로토콜은 종단 시스템에서 구현된다. 트랜스포트 계층에서는 애플리케
      이션 프로세스로부터 받은 메시지를 세그먼트라는 단위의 패킷으로 변환 하고 이때 메시지를
      조각으로 분할하여 헤더를 추가함으로써 수행된다. 이후 네트워크 계층으로 전달되어 캡슐화 후
      목적지로 전달된다.
    </p>
    <h1>트랜스포트 계층과 네트워크 계층 사이의 관계</h1>
    <p>
      트랜스포트 계층 프로토콜은 서로 다른 호스트에서 동작하는 프로세스들 사이의 논리적 통신을
      제공하지만 네트워크 계층 프로토콜은 호스트들 사이의 논리적 통신을 제공한다.
    </p>
    <h1>인터넷 트랜스포트 계층의 개요</h1>
    <p>
      개발자는 소켓의 생성 시에 TCP와 UDP 중에 하나를 선택해야 한다. 네 트워크 계층의 프로토콜
      ‘인터넷 프로토콜’은 IP라고 부르는데 이 서비스 모델은 호스트들 간에 논리적 통신을 제공하는
      최선형 전달 서비스이다. IP는 호스트들 간의 세그먼트 전달을 위해 최대한 노력하지만 순서나 신
      뢰적 전송에 대한 보장 서비스를 제공하지 않기에 비 신뢰적 서비스라고 부른다. TCP와 UDP의 가장
      기본적인 기능은 종단 시스템 사이의 IP 전 달 서비스를 두 프로세스 간의 전달 서비스로 확장하는
      것이다. 이처럼 호스트-호스트 전달을 프로세스-프로세스 전달로 확장하는 것을
      <span>트랜스 포트 다중화</span>와 <span>역다중화</span>라고 부른다. 트랜스포트 계층
      프로토콜들은 헤 더에 오류 검출 필드를 포함함으로써 무결성 검사를 제공한다. UDP가 제공하는
      유일한 두가지 서비스가 위에 말한 다중화와 무결성 검사이다. TCP는 추가적인 서비스를 제공하는데
      첫 번째로 <span>신뢰적 데이터 전달</span> 을 보장한다. 이는 흐름제어, 순서번호, 확인응답,
      타이머를 사용함으로써 데이터 전송을 확실히 한다. 이로써 TCP는 비신뢰적 서비스인 IP를 신 뢰적
      서비스로 이용하도록 해준다. 또한 TCP의 <span>혼잡 제어</span> 는 애플리케이 션에서 야기 시켜서
      제공되는 서비스가 아닌 전체를 위한 서비스로 네트 워크상에 과도한 양의 트래픽으로 스위치와
      링크가 폭주하는 것을 방지 하는 것으로 송신 측 TCP가 네트워크에 보낼 수 있는 트래픽을 조절함
      으로써 수행된다. 반면 UDP는 트래픽을 조절하지 않는다.
    </p>
    <h1>다중화와 역다중화</h1>
    <p>
      목적지 호스트에서의 트랜스포트 계층은 네트워크 계층으로부터 세그먼 트를 수신하는데 이를
      애플리케이션 계층으로 전달할 의무를 가지고 있 다. 수신자 측 호스트의 트랜스포트 계층은 실제로
      데이터를 직접 프로 세스로 전달하지 않고 중간 매개자인 소켓에게 전달한다. 이때 여려 소 켓이
      있을 수 있으므로 각 소켓은 식별자를 가진다. 수신 측 호스트가 수신한 세그먼트를 올바른 소켓에게
      보내기 위해 세그먼트 필드 집합을 가지고 있는데 트랜스포트 계층은 수신 소켓을 식별하기 위해
      이들 필드 를 검사한다. 이처럼 트랜스포트 계층 세그먼트의 데이터를 올바른 소켓 으로 전달하는
      작업을 <span>역다중화</span>라고 한다. 반대로 출발지 호스트에서 소켓으로부터 데이터를 모으고
      세그먼트 생성을 위해 각 데이터에 헤더 정보로 캡슐화하고 그 세그먼트를 네트워크 계층으로
      전달하는 작업을 다 중화라고 한다. 다중화를 위해서는 두 가지 조건이 있는데
      <span>소켓은 유일한 식별자</span> 를 가져야하고 각 세그먼트는 전달될 적절한 소켓을 가리키는
      <span>특별한 필드</span>를 가져야 한다는 것이다. 이때 특별한 필드라는 것은 출발 지 포트 번호
      필드와 목적지 포트 번호 필드인데 각각 16비트 정수(0 ~ 65535)로 이루어져 있으며 그 중에서 0 ~
      1023까지 포트 번호를 <span>잘 알 려진 포트 번호</span>라고 하여 사용을 엄격하게 제한하는데
      HTTP(포트 번호 80)와 FTP(포트 번호 21)처럼 잘 알려진 애플리케이션 프로토콜에서 사용되도록
      예약되어 있다. 새 애플리케이션을 개발할 때는 포트 번호를 반드시 할당해야 한다.
    </p>
    <h1>비연결형 다중화와 역다중화</h1>
    <p>
      UDP에서 데이터 전송 시 송신 측 호스트의 트랜스포트 계층에서 세그 먼트를 생성할 때 출발지 포트
      번호 즉, 송신 측 호스트의 소켓의 정보 와 수신 측 호스트의 소켓 정보를 담고 네트워크 계층으로
      보내게 되고 수신 측에 도착한 세그먼트는 수신 측 호스트가 목적지 포트를 검사하여 세그먼트를
      적절한 소켓으로 보낸다.(역다중화) 이때
      <span>UDP 소켓은 목적지 IP주소와 목적지 포트 번호만으로 소켓을 향하는 것이 특징이다.</span>
    </p>
    <h1>연결지향형 다중화와 역다중화</h1>
    <p>
      TCP 소켓과 UDP 소켓 사이의 다른 점은 TCP 소켓은 4개의 요소들의 집합으로 이루어져 있는데 출발지
      IP주소, 출발지 포트 번호, 목적지 IP 주소, 목적지 포트 번호에 의해 식별된다는 것이다.
      <span
        >수신 측 호스트에 TCP 세그먼트가 도착하면 호스트는 해당 소켓으로 세그먼트를 전달하기 위해서
        4개의 값을 모두 사용하는</span
      >데 만약 출발지 포트 번호 또는 IP 주 소가 다르다면 서로 다른 소켓에 전달된다. 예를 들어 한
      호스트에서 서 버로 두 개의 HTTP 연결을 시도한다면 출발지 포트 번호를 통해 올바 른 소켓으로
      보내는 역다중화가 가능하다.
    </p>
    <h1>비연결형 트랜스포트: UDP</h1>
    <p>
      UDP는 트랜스포트 계층 프로토콜이 할 수 있는 최소 기능으로 동작하 는데 다중화/역다중화 기능과
      간단한 오류 검사 기능을 제외하면 IP에 아무것도 추가하지 않으며 거의 IP와 직접 통신하는 셈이다.
      DNS는 일 반적으로 UDP를 사용하는 애플리케이션 계층 프로토콜의 예인데 호스트 에서 DNS
      애플리케이션이 질의를 생성할 때 DNS 질의 메시지를 작성 하고 UDP에게 메시지를 넘겨주면 메시지에
      헤더 필드를 추가하고 네트 워크 계층에 넘겨주어 데이터그램으로 캡슐화하고 네임 서버에 데이터그
      램을 송신한다. 많은 애플리케이션이 UDP에 적합한 이유는 다음과 같 다. 첫째로 무슨 데이터를 언제
      보낼지에 대해 애플리케이션 차원에서 더 정 교하게 제어가 가능하다. UDP 방식으로 데이터를
      송신하게 되면 프로세 스가 데이터를 UDP에 전달하자마자 세그먼트로 만들고 네트워크 계층으 로
      바로 전달한다. 반면에 TCP는 혼잡제어 메커니즘을 가지고 있어 전 송 속도에 제한이 걸릴 가능성이
      크다. 실시간 애플리켕션의 경우 최소 전송률을 요구하고 지연을 원치 않기 때문에 TCP 보다 UDP에
      더 적합 하며 추가적으로 UDP의 기본 세그먼트 외에 어떠한 추가 기능을 구현 할 수 있다. 둘째로
      연결 설정이 없다. TCP는 세 방향 핸드셰이크를 사용하므로 서 버에서의 동작이 느려질 수 있는
      반면에 UDP는 그렇지 않다. HTTP 문 서로 된 웹페이지의 경우 신뢰성이 중요하므로 TCP를 사용하지만
      HTTP에서 TCP 연결은 웹 문서의 다운로드 지연에 심각한 공헌자이다. 셋째로 연결 상태가 없다.
      TCP는 종단 시스템에서 연결 상태를 유지한 다. 이 연결 상태는 수신 버퍼 및 송신 버퍼, 혼잡 제어
      파라미터, 순서 번호와 확인응답 번호 파라미터를 포함한다. UDP는 이 파라미터 중의 어떤 것도
      기록하지 않기 때문에 특정 애플리케이션에 할당된 서버는 애 플리케이션이 UDP에서 동작할 때 좀 더
      많은 클라이언트를 수용할 수 있다. 넷째로 TCP가 세그먼트마다 20바이트의 헤더 오버헤드를 갖는
      반면에 UDP는 8바이트의 오버헤드를 가진다. 위의 장점이 있듯 단점도 존재한다. UDP는 혼잡제어를
      기능이 없으므 로 만약 모두가 혼잡제어를 사용하지 않는다면 네트워크가 폭주할 가능 성이 높고
      라우터에서 많은 패킷 오버플로우가 발생할 것이며 소수의 패 킷만이 도착하여 UDP 패킷의 손실률이
      매우 높아질 것이다. 이를 위해 적응식 혼잡제어 수행을 위해 새로운 메커니즘을 제안했는데
      트랜스포트 프로토콜이 아닌 애플리케이션 자체가 전송에 대한 신뢰성을 보장한다면 TCP의 혼잡제어
      메커니즘에 전송률을 억제당하지 않고 신뢰적 통신이 가능할 것이다.
    </p>
    <h1>UDP 세그먼트 구조</h1>
    <p>
      UDP 세그먼트 구조에서 애플리케이션 데이터는 UDP 데이터그램의 데 이터필드에 위치하는데 DNS에
      대한 데이터 필드에는 지르이 메시지나 응답 메시지를 포함한다. UDP 헤더는 2바이트씩 구성된 4개의
      필드를 가지는데 출발지 포트 번호, 목적지 포트 번호, 길이, 체크섬 등이 있다.
    </p>
    <h1>UDP 체크섬</h1>
    <p>
      링크 계층 프로토콜이 오류 검사 서비스를 제공함에도 체크섬이 오류 검 사 서비스를 제공하는
      이유는 출발지와 목적지 사이의 모든 링크가 오류 검사를 제공한다는 보장이 없기 때문이다. 예를
      들어 링크를 통해 정확 한 세그먼트가 전달 되었더라도 라우터의 메모리에 저장될 때 비트 오류 가
      발생하는 것이 가능하다. 그렇기 때문에 종단간의 데이터 전송 서비 스가 오류 검사를 제공한다면
      UDP는 종단 간의 트랜스포트 계층에서 오류 검사를 제공해야만 하는데 이를 종단 간 원리의 한
      예이다. 즉, 어 떤 기능을 종단 기반으로 구현해야 하므로 하위 레벨에 위치한 기능들은 상위
      레벨에서 이들을 제공하는 비용을 비교했을 때 중복되거나 유용하 지 않을 수 있다. IP의 경우 어떤
      2계층의 프로토콜에서도 동작해야 하 므로 트랜스포트 계층은 안전장치로써 오류 검사를 제공하는데
      이를 회 복하는 어떤 기능도 가지고 있지 않다. 일부 UDP는 손상된 세그먼트를 버리거나 손상된
      세그먼트를 애플리케이션에게 경고와 함께 넘겨준다.
    </p>
    <h1>신뢰적인 데이터 전달 프로토콜의 구축 유한상태머신(FSM)</h1>
    <p>
      완벽하게 신뢰적인 채널 상의 신뢰적인 데이터 전송 -완전히 신뢰적인 채널에서는 오류가 생길 수
      없으므로 수신 측이 송신 측에게 어떤 피드백도 제공할 필요가 없으며 수신자는 송신자가 데이터 를
      송신하자마자 데이터를 수신할 수 있다고 가정한다. 비트 오류가 있는 채널 상에서의 신뢰적 데이터
      전송: rdt2.0 -하위 채널의 더 실질 모델은 패킷 안의 비트들이 하위 채널에서 손상되 는 모델이다.
      패킷이 전송 또는 전파되거나 버퍼링 될 때 네트워크의 물 리적 구성 요소에서 일반적으로 발생하며
      전송된 모든 패킷들이 송신된 순서대로 수신된다고 가정한다. 이 경우 신뢰적인 통신을 위해
      프로토콜 은 <span>긍정 확인 응답</span>과 <span>부정 확인 응답</span>을 사용한다. 이런 제어
      메시지는 송신자에게 수신자의 수신 여부를 확인할 수 있게 해준다. 컴퓨터 네트 워크에서 이러한
      재전송을 기반으로 하는 신뢰적인 데이터 전송 프로토 콜은
      <span>자동 재전송 요구 프로토콜(ARQ)</span>로 알려져 있다. 비트 오류를 해 결하기 위해 세가지
      부가 프로토콜 기능들이 ARQ 프로토콜에 요구된다. 오류 검출: 비트 오류가 발생했을 때 수신자가
      검출할 수 있어야 하는 기능이 필요하다. UDP는 이 목적을 위해 인터넷 체크섬 필드를 사용한 다.
      오류를 검출하고 복구할 수 있도록 해주는 이 기술은 본래 송신자가 수신자에게 전달해야 하는 그
      이상의 비트를 요구한다. 비트들은 rdt2.0 데이터 패킷의 패킷 체크섬 필드로 모아진다. 수신자
      피드백: 송신자와 수신자가 일반적으로 멀리 떨어진 종단 시스템 에서 동작하므로 송신자가 수신
      상태를 알기 위한 유일한 방법은 피드백 이다. <span>긍정 확인응답(ACK)</span>와
      <span>부정 확인응답(NAK)</span>이 피드백의 예이다. 이는 한 비트로 나타낼 수 있다.(0과 1)
      재전송: 수신자에서 오류를 가지고 수신된 패킷은 송신자에 의해서 재전 송 된다. 송신자 측에서
      패킷을 보내고 수신자로부터 ACK 또는 NAK 패킷을 기 다리는데 ACK가 수신된다면 가장 최근에 전송된
      패킷이 정확하게 수신 되었다는 것을 알게 되고 프로토콜은 상위 계층으로부터 데이터를 기다 린다.
      만약 NAK가 수신되면 프로토콜은 마지막 패킷을 재전송하고 재전 송 된 데이터 패킷에 대한 응답으로
      수신자에 의해 응답 되는 ACK 또 는 NAK를 기다린다. 여기서 중요한 점은 송신자가 수신자의 응답을
      기 다릴 때 상위 계층으로부터 더 이상의 데이터를 받을 수 없다는 것이다. 이처럼 송신자가
      수신자의 응답 이전에 새로운 패킷을 전달하지 않기 때 문에 rdt2.0과 같은 프로토콜은
      <span>전송 후 대기 프로토콜</span> 로 알려져 있다. rdt2.0도 결함이 있는데 ACK 또는 NAK 패킷이
      손상될 가능성을 고려 하지 않은 것이다. 이를 처리하기 위한 세 가지 가능성있는 방법은 다음 과
      같다. 송신자 대 수신자 패킷의 새로운 타입 : 응답의 손상 여부를 다시 확인 하는 패킷을 보내는
      것인데 이마저 손상된다면 답이 없어진다. 송신자가 체크섬 비트 추가 : 송신자가 전송 시 비트 오류
      검출 및 회복 할 수 있도록 충분한 체크섬 비트를 추가하는 것인데 패킷이 손실 되지 않는 채널의
      경우 바로 해결이 가능하다. 똑같은 패킷 재전송 : 왜곡된 ACK 나 NAK 패킷을 수신할 때 다시 재
      전송 하는 방법인데 응답이 ACK인지 NAK인지 구분할 수 없으므로 재 전송한 패킷이 새로운
      데이터인지 재전송인지 수신자가 알 수 없다. 위의 부가적인 프로토콜의 문제점 해결을 위한 간단한
      해결책은 데이터 패킷에 새로운 필드를 추가하고 순서번호를 삽입하는 방식으로 데이터 패킷에
      송신자가 번호를 매기는 것이다. 수신자는 재전송 될 패킷의 순 서 번호만 확인하면 되고 특히 전송
      후 대기 프로토콜의 경우 한 비트 순서번호면 송신자가 재전송할 것인지 새로운 패킷을 보낼 것인지
      정할 수 있다.
    </p>
    <h1>비트 오류,손실 있는 채널에서의 신뢰적 데이터 전송 rdt3.0</h1>
    <p>
      비트가 손상되는 것 외에도 컴퓨터 네트워크처럼 하위 채널이 패킷 손실 이 발생하는 경우이다. 두
      가지 내용이 추가되어야 하는데 어떻게 패킷 손실을 검출할 것인가, 손실 발생 시 어떻게 처리할
      것인지에 대한 프로 토콜이 필요하다. 손실 발생 시 처리 방법에는 체크섬, 순서번호, ACK패킷,
      재전송 등을 통해 해결할 수 있다. 그러나 패킷 손실의 검출은 송신자가 보낸 패킷에 대한 응답
      여부로 확인할 수 있다. 송신자는 적어도 송수신자의 왕복시 간 지연에 수신 측에서 패킷 처리 시
      걸리는 시간을 기다려야 하는데 이 는 예측하기 어렵다. 따라서 일정 시간이 지난 후에는 중복된
      패킷을 보 내는데 이것은 송신자 대 수신자 채널에서 중복 데이터 패킷의 가능성을 포함한다. 이미
      rdt2.2에서 중복 패킷에 대한 처리를 순서번호로 가능하 게 하였으므로 송신자에게 주어진 시간 내에
      응답이 온다면 재전송을 인 터럽트 할 수 있는 <span>카운트다운 타이머</span>가 필요하다.
      rdt3.0프로토콜은 패 킷의 순서 번호가 0과 1이 번갈아 일어나므로
      <span> 얼터네이팅 비트 프로토 콜</span>이라고도 한다.
    </p>
    <h1>파이프라인 된 신뢰적 데이터 전송 프로토콜</h1>
    <p>
      rdt3.0은 기능적으로 정확한 프로토콜이지만 고속 네트워크에서 만족스 러운 성능이 나오지는
      않는다. 따라서 전송 후 대기 방법으로 동작하는 대신 송신자에게 ACK를 기다리지 않고 여러 패킷을
      전송하도록 허용하 고 추후 확인응답이 오지 않은 패킷만 재전송 한다면 시간적 효율을 높 을 수
      있다. 이는 많은 패킷을 파이프라인에 채워 넣음으로써 나타낼 수 있으며 이 기술을
      <span>파이프라이닝</span>이라고 부른다. 파이프라이닝 방식은 신 뢰적인 데이터 전송 프로토콜에서
      다음과 같은 중요성을 가지고 있다. -순서 번호의 범위가 커져야 하며 각 전송 중인 패킷은 유일한
      순서 번 호를 가져야 한다. -프로토콜의 송신 측과 수신 측은 한 패킷 이상을 버퍼링해야 한다. 즉,
      확인된 패킷과 확인되지 않은 패킷을 임시 저장하여 <span>N부터 반복이나 선 택적 반복</span>등과
      같은 파이프라인 오류 회복에 사용해야 한다.
    </p>
    <h1>N부터 반복(Go-Back-N, GBN)</h1>
    <p>
      GBN 프로토콜에서 송신자는 확인응답을 기다리지 않고 여러 패킷을 전 송할 수 있다. 단, 패킷의
      최대허용수 N을 넘으면 안된다. 응답이 아직 오지 않은 패킷 순서번호를 base로 정의하고 다음 보낼
      예정인 패킷의 순서 번호를 nextseqnum으로 정의한다면 [0~base-1]까지는 전송에 성 공한 패킷의
      순서번호이고 [base~ nextseqnum-1]은 전송은 하였으나 응답이 오지 않은 패킷의 순서번호이다.
      [nextseqnum ~ base + N –1] 은 상위 계층에서 데이터를 보내주면 바로 전송될 수 있는 패킷이고
      base + N 이상의 순서번호를 가진 패킷은 base 순서번호를 가진 패킷 부터 그 뒤에 전송한 패킷들이
      응답이 오지 않으면 보낼 수 없는 패킷들 이다. 이때 N은
      <span>윈도우 크기(window size)</span>라고 부르며
      <span>슬라이딩 윈도우 프로토콜(sliding-window protocol)</span>이라고도 한다. 추가로 N의 크기를
      제한하는 이유는 TCP의 서비스 중 하나인 혼잡 제어의 영향이다. <span>누적 확인응답</span>:
      GBN프로토콜에서 순서번호 n을 가진 패킷에 대한 확인 응답은 누적 확인응답으로 인식되는데 그
      이유는 수신자 측에서 n에 대 한 패킷이 올바르게 순서대로 수신했을 경우 상위 계층에 전달하며 그
      외의 경우에는 패킷을 버리고 올바르게 수신한 마지막 패킷에 대한 ACK 를 전송하기 때문이다.
      타임아웃이벤트: 전송 후 대기 프로토콜과 같이 타이머는 손실된 패킷 및 확인응답 패킷에 대한
      회복에 사용되며 일정 시간 후 확인응답이 오 지 않은 패킷부터 재전송을 시작한다. 재전송 시
      타이머는 다시 작동한 다. GBN 프로토콜에서는 순서가 잘못된 패킷들을 버리는데 이는 상위 계층 에
      데이터를 전달해야 하기 때문이다. 또한 n의 순서번호를 가진 패킷부 터 재전송하는 프로토콜이므로
      올바른 패킷을 받아도 그 전에 받은 패킷 에 문제가 있다면 다시 재전송해야 하는 이유가 있다.
      +<span>이벤트 기반 프로그래밍</span> : FSM, 프로토콜의 구현 등과 같이 다양한 이 벤트에 대한
      대응을 취할 수 있는 동작을 구현하는 프로시저
    </p>
    <h1>선택적 반복(SR)</h1>
    <p>
      GBN에서 윈도우 크기 N이 클 때 하나의 패킷에 오류가 난다면 많은 패 킷을 불필요하게 재전송해야
      하는 경우가 발생하는데 SR 프로토콜은 수 신자에게 오류가 발생한 패킷을 수신했다고 의심되는
      패킷만을 재전송한 다. 다음은 SR 프로토콜에서 송신자의 이벤트 및 동작이다. 타임아웃 : 타이머는
      손실된 패킷을 보호하기 위해서 사용되며 각 패킷 은 자신의 논리 타이머를 가져야 한다. ACK 수신 :
      ACK가 수신되었을 때 순서번호가 맞는 패킷이 윈도우 N 안에 있다면 응답받은 것으로 표시하고
      응답이 오지 않은 패킷 중 가장 순서 번호가 작은 것으로 윈도우 베이스를 이동시키고 재설정된
      윈도우 범위 안에 미응답인 패킷을 재전송한다. 다음은 SR 프로토콜에서 수신자의 이벤트 및
      동작이다. 수신자는 패킷의 순서번호와 상관없이 올바른 패킷에 대한 ACK 응답을 보내고 만약 중간에
      손실된 패킷이 있다면 대기하였다가 손실된 패킷이 잘 수신된 시점에서 패킷들을 순서대로 상위
      계층에 전달한다. SR 프로토콜에서 발생할 수 있는 문제 중 하나는 송신자의 윈도우와 수신자의
      윈도우가 다르다는 것에서 발생할 수 있는데 교재의 예시와 같 이 순서번호가 중복되었고 우연히
      손실된 패킷과 같은 순서번호를 가진 패킷이 도착한다면 처음 손실된 패킷인지 새로운 패킷인지
      구별할 수가 없다. 따라서
      <span
        >윈도우 크기는 SR 프로토콜에 대한 순서번호 공간 크기의 절반보다 작거나 같아야 한다.</span
      >
    </p>
    <h1>연결지향형 트랜스포트: TCP</h1>
    <p>
      앞에서 했던 신뢰적 데이터 전송을 위한 rdt, GBN, SR등의 원칙들을 따 르고 있는 것이 TCP이다. TCP
      연결은 두 종단 시스템에서만 존재하므 로 중간에 라우터 등 네트워크 요소들은 연결 상태를
      인식하지 못한다. <span>TCP 연결은 전이중 서비스를 제공</span>한다. 즉, 데이터의 흐름이
      양방향이 라는 것이고 단일 송신자 수신자의 <span>점대점 연결</span>이므로 단일 송신자에서 여러
      수신자에게 데이터 전달을 하지 못한다. 연결을 초기화하는 프로세 스는 클라이언트 프로세스, 다른
      프로세스는 서버 프로세스이다. 핸드셰 이크 과정에서 주고 받는 첫 번째와 두 번째 세그먼트는
      <span>페이로드(애플 리케이션 계층 데이터)</span>가 없다. 세 번째 세그먼트는 페이로드를 포함할
      수 있다. 이후 데이터 전달 과정은 다음과 같다. 송신자 프로세스가 소켓을 통해서 데이터를
      전달하면 TCP에 맡겨지고 핸드셰이크 동안 준비된 버퍼의 하나인 연결의 송신 버퍼로 데이터를 보
      낸다. 이때 데이터 묶음을 만들어서 보내는데 이때 세그먼트의 최대 크 기는
      <span>MSS(최대 세그먼트 크기)</span>로 제한되며 이는 MTU(최대 전송 단위, 가장 큰 링크 계층
      프레임의 길이)로 우선 결정되고 TCP세그먼트 + TCP/IP 헤더 길이가 단일 링크 계층 프레임에 딱
      맞도록 하여 정해진 다. MSS는 헤더가 붙기 전 애플리케이션 계층 데이터에 대한 최대 크기 이다!!
      TCP는 TCP 헤더와 클라이언트 데이터를 하나로 만들어서 TCP 세그먼트를 형성하고 네트워크 계층에
      전달하고 IP 데이터그램 안에 캡 슐화되어 전달된다. 이때 TCP 전송 버퍼는 TCP 수신 버퍼와
      연결된다.
    </p>
    <h1>TCP 세그먼트 구조</h1>
    <p>
      TCP 세그먼트는 헤더 필드와 데이터 필드로 구성되어 있는데 데이터필 드는 한 줌의 애플리케이션
      데이터를 담는다. MSS는 세그먼트의 데이터 필드 크기를 제한하는데 TCP가 웹 문서의 이미지와 같은
      큰 파일 전송 시에는 MSS 크기로 파일을 쪼갠다.
      <span>
        -출발지와 목적지 포트 번호 각 16비트(헤더) -체크섬 필드 16비트 -순서번호 필드, 확인응답 번호
        필드 각 32비트 -헤더 길이 4비트 -옵션 -플래그 필드 6비트 -긴급 데이터 포인트 필드
        16비트</span
      >
      TCP의 세그먼트에 대한
      <span>순서번호는 세그먼트에 있는 첫 번째 바이트의 바이트 스트림 번호</span>이다. (예시.
      500바이트 전송 시 MSS 10바이트, 순 서번호는 0, 10, 20 ...) 순서번호는 연결 시 임의로
      선택하는데 이는 종료된 연결로부터 네트워크에 남아있는 세그먼트가 나중 연결에서 유효한
      세그먼트로 오인될 확률을 최소화하기 위함이다. 확인응답 번호는 TCP가 전이중 방식임을 유의해야
      한다. 만약 호스트 A 가 패킷을 송신함과 동시에 수신 중이라면 A가 수신하지 못한 바이트의
      시작점을 호스트 B에게 알려주는 용도이며 만약 중간 부분이 도착하지 않았음에도 마지막 부분이
      들어왔다면 확인응답 번호에는 도착하지 않은 중간 부분의 바이트까지만 확인응답을 한다. 이로 인해
      TCP는 <span>누적 확인응답</span>을 제공한다. 만약 패킷이 순서대로 수신되지 않았을 경우 손실된
      부분을 제외한 세그먼트는 어떻게 처리하는지는 구현자의 선택이다. 순 서가 틀린 세그먼트를 버려
      설계를 단순화할 수도 있고 버리지 않고 보 유함으로써 네트워크 밴드폭을 넓히는 방법도 있다.
      +<span>피기백(piggy-backed)</span>된다의 의미 : 클라이언트와 서버 간의 확인응답 은 데이터를
      운반하는 세그먼트 안에서 전달된다는 것이다.
    </p>
    <h1>-왕복시간 예측</h1>
    <p>
      TCP는 sampleRTT라고 표시되는 세그먼트에 대한 RTT(Round Trip TIme) 샘플은 세그먼트가 송신된
      시간부터 긍정응답이 도착한 시간까지 를 의미하는데 어느 시점마다 전송된 세그먼트에 대해
      왕복시간을 측정 한다. 또한 재전송한 세그먼트에 대한 sampleRTT는 계산하지 않는다. 이 값은
      라우터에서의 혼잡과 종단 시스템에서의 부하 변화 때문에 세그 먼트마다 다르게 측정되는데 이것의
      평균값을 계산해 채택한다. SampleRTT의 값이 가중된 값은 EstimatedRTT라고 하며 공식은 다음 과
      같다. <span>EstimatedRTT = 0.875 * EstimatedRTT + 0.125 * SampleRTT</span> 즉 EstimatedRTT는
      가중 평균이다. 이 가중 평균은 예전 샘플보다 최근 샘플에 높은 가중치를 주는데 이는 최근의
      샘플이 현재의 네트워크 혼잡 상태를 잘 반영하기 때문이다. 이러한 평균을
      <span> 지수적 가중 이동 평 균(EWMA)</span>라고 부른다. DevRTT는 SampleRTT와 EstimatedRTT 값
      차이의 EWMA로 SampleRTT의 변동에 비례한 크기를 가진다. 재전송 타임아웃 주기의 설정은 다음과
      같다. <span> TimeoutInterval = EstimatedRTT + 4*DevRTT</span>와 같으며 초기 TimeoutInterval은
      1초를 권고한다. 만약 타임아웃이 발생하면 TimeoutInterval은 2배로 늘리고 발생하지 않고
      세그먼트가 수신되어 EstimatedRTT가 수정되면 다시 위의 공식에 따라 계산한다.
    </p>
    <h1>신뢰적 데이터 전달</h1>
    <p>
      네트워크 계층의 IP 프로토콜 서비스는 비신뢰적이다. 그러나 TCP는 비 신뢰적인 최선형 서비스에서
      신뢰적인 데이터 전달 서비스를 제공한다. 다음은 TCP 프로토콜에서 일어날 수 있는 몇 가지
      시나리오이다. -세그먼트를 보냈을 때 긍정 확인응답이 손실된 경우 : 타임아웃이 발 생하면
      송신자는 재전송하게 되고 수신자는 이미 수신한 세그먼트이므 로 재전송된 바이트를 버리고
      확인응답을 다시 보낸다. -연속해서 세그먼트를 보냈을 경우 : 첫 번째 세그먼트에 대한 확인응 답이
      타임아웃 이벤트가 발생했을 때 재전송하게 되고 다시 타이머가 돌아가며 그러던 중 첫 번째와 두
      번째 패킷에 대한 응답이 도착한다 면 두 번째 세그먼트에 대한 재전송은 하지 않으며 수신자 역시
      첫 번 째 세그먼트의 재전송이 오더라도 확인응답을 두 번째 세그먼트에 대 한 확인응답을 해준다. -
      연속해서 세그먼트를 보냈을 경우 : 만약 첫 번째 세그먼트에 대한 확 인응답이 분실되고 두 번째
      세그먼트에 대한 확인응답이 온다면 송신 자는 수신자가 두 개의 세그먼트 모두 수신했다는 것을
      알게 되므로 재전송 하지 않게 된다. - 타임아웃 주기의 두 배 설정 : 타임아웃 이벤트가 발생할
      때마다 TimeoutInterval은 2배씩 증가하는데 만약 상위 애플리케이션으로부 터 데이터 수신이
      일어나거나 ACK, 긍정 확인응답이 온다면 가장 최 근의 값을 기준으로 설정된다. 이는 혼잡제어의 한
      부분이기도 하며 병목지점인 라우터에 계속해서 같은 패킷을 보낸다면 혼잡이 더 악화 되므로 전송
      간격을 커지게 하는 것이다. -빠른 재전송 : 타임아웃 주기가 너무 길면 송신자의 대기가 길어지고
      종단 간의 지연을 증가시키는데 이는 중복 ACK에 의해 타임아웃 이 벤트가 발생하기 전 재전송을
      하기도 한다. 중복 ACK를 보내는 이유 는 TCP는 NAK를 사용하지 않기 때문이다.(TCP는 누적
      확인응답, 선 택적 확인 응답 방식 중 선택할 수 있는데 이는 GBN과 SR의 차이로 보면 되며 책에서는
      GBN을 토대로 설명하는 것 같음) 중복 ACK는 TCP 수신자의 ACK 생성 정책에 따라 만들어지는데 이는
      아래를 확 인. 만약 송신자가 많은 양의 세그먼트를 연속적으로 보낼 수 있으므로 만 약 하나의
      세그먼트를 잃어버린다면 그 뒤에 온 잃어버리기 전 수신한 세그먼트에 대한 ACK가 중복되어
      송신자에게 보내질 것이다. 이는 손실 된 세그먼트의 타임아웃 이벤트 발생 전에 재빨리
      전송함으로써 효율성 을 높일 수 있다.
    </p>
    <h1>TCP의 오류 검출</h1>
    <p>
      TCP는 GBN, SR의 방식 모두 사용하고 있다. GBN의 경우 수신자 측에 서도 순서가 잘못된 세그먼트를
      버리지만 반면에 TCP는 버퍼에 저장하 고 손실된 세그먼트에 대한 중복 ACK를 보낸다. 송신자 측에서
      볼 때 확인응답이 손실되거나 타임아웃 이벤트가 발생하면 GBN의 경우 손실된 세그먼트부터 다시
      전송하지만 TCP의 경우 손실된 세그먼트만 보내거나 이후 순서번호를 가진 세그먼트에 대한 확인
      응답이 올 경우 다시 보내 지 않는다. 이처럼 .TCP는 두 가지 성격 모두를 띄고 있다.
    </p>
    <h1>흐름제어</h1>
    <p>
      TCP에서 각 종단에서 호스트 연결에 대한 개별 수신 버퍼를 설정한다. 이때 애플리케이션 프로세스는
      버퍼에서 데이터를 읽는다. 그러나 애플 리케이션이 다른 작업으로 인해 오랜 시간 동안 버퍼에 있는
      데이터를 읽지 않거나 데이터의 수신 속도보다 느리게 읽으면 버퍼에 오버플로우 가 발생한다. 이를
      방지 하기 위한 것이 <span>흐름제어</span> 서비스이다. 이 서비 스는 수신하는 애플리케이션이
      읽는 속도와 송신자가 전송하는 속도를 맞추어 오버플로우가 일어나지 않도록 한다. 다음은
      예시이다. TCP는 송신자가 <span>수신 윈도우(receive window)</span>라는 변수를 유지하여 흐
      름제어를 제공하는데 수신 윈도우가 수신 측의 가용한 버퍼 공간을 송신 자에게 알려줄 때 사용된다.
      TCP는 전이중이므로 각 측의 송신자는 별 개 수신 윈도우를 유지한다. 수식은 다음과 같다. rwnd =
      수신 측 버퍼 여유 공간 LastByteRead = 애플리케이션이 마지막으로 읽은 바이트 LastByteRcvd =
      수신 버퍼에 저장된 마지막 바이트 RcvBuffer = 수신 버퍼 크기 rwnd = RcvBuffer – [LastByteRcvd –
      LastByteRead] LastByteSent = 송신 측이 보낸 마지막 바이트 LastByteAcked = 송신 측이 받은
      마지막 확인응답 수신 측을 B, 송신측을 A라고 가정했을 때 TCP 연결이므로 B가 A에게 세그먼트를
      보낼 때 rwnd의 값을 보내주고 A는 송신 측이 가지는 위의 두 가지 변수를 유지함으로 B의 버퍼에
      오버플로우가 발생할지에 대한 여부를 알 수 있다. 그러나 이 방식에 문제점이 있는데 만약 B의 rwnd
      값이 0이 되었다는 것을 A에게 알린 후 더 보낼 세그먼트가 없다면 (TCP는 전송할 데이터나 확인
      응답이 있을 때만 세그먼트를 보냄) B의 애플리케이션 프로세스가 버퍼를 비우더라도 A가 확인할
      방법이 없다. 이 문제를 해결할 방법이 TCP 명세서에 나와 있는데 만약 B의 수신 윈 도우가 0이 되면
      0값을 1바이트 데이터로 세그먼트를 계속해서 보내고 만약 버퍼가 비워진다면 0이 아닌 rwnd 값을
      보내는 것이다.
    </p>
    <h1>TCP 연결 관리</h1>
    <p>
      TCP 연결 설정 과정은 다음과 같다. - 3 way handshake 1. 클라이언트에서 서버로 특별한 세그먼트를
      송신하는데 여기에는 애 플리케이션 계층 데이터가 없으며 헤더에 SYN 비트라고 불리는 하나의
      플래그 비트를 가진다.(SYN 세그먼트) 2. 서버는 SYN 세그먼트를 받고 TCP 수신 버퍼와 변수들을
      할당하며 SYNACK 세그먼트를 보내는데 이때 SYN 비트는 1로 설정되고 헤더의 확인응답 필드는
      client_isn+1 값을 가지며 서버 자신의 최초의 순서번 호를 생성해 헤더의 순서번호 필드에 추가한
      후 회신한다. 3. 클라이언트가 SYNACK 세그먼트를 수신하면 연결에 버퍼와 변수 들을 할당하고
      서버에게 세그먼트 헤더의 확인응답 필드 안에 server_isn+1 값을 넣어 보냄으로써 연결이 설정된다.
      또한 세그먼트 페이로드에 애플리케이션 계층의 데이터도 추가가 가능하다. 둘 중 어느 프로세스든지
      연결을 해제할 수 있고 연결이 끝날 때 버퍼 와 변수들은 회수된다. 종료할 때도 특별한 세그먼트를
      송신하는데 1로 설정된 FIN 비트라 불리는 플래그 비트를 헤더에 포함한다. 그 후 똑같 이 FIN
      비트를 1로 설정한 값을 가진 헤더를 회신하고 다시 확인응답을 한 후에 자원에 대한 할당이
      해제되며 연결이 종료된다. 위의 설명은 TCP 연결에 대한 준비가 다 되어있다는 가정이다. 만약 80번
      포트로 연결을 요청했지만 사용할 수 없는 상태라면 리셋 세그먼 트(RST 플래그 비트)를 회신한다.
    </p>
    <h1>혼잡제어의 원리</h1>
    <p>
      예시1. 2개의 송신자와 무한 버퍼를 갖는 하나의 라우터 수신자 측 처리량은 송신자 측 전송률과
      같으며 호스트당 최대 전송률은 R/2이며 이를 초과한 전송/처리량이 될 수 없다. 만약 R/2를
      초과하게 되면 평균 지연이 무제한으로 커진다. 예시2. 2개의 송신자와 유한 버퍼를 갖는 하나의
      라우터 트랜스포트 계층에서의 송신율은 네트워크에 <span>제공된 부하</span>라고 부른다. 버퍼가
      유한하므로 패킷 로스가 발생하지만 재전송하게 되고 이로 인해 혼잡 네트워크의 비용이 발생한다.
      즉, 송신자의 버퍼 오버플로우 때문 에 버려진 패킷에 대한 재전송 비용이 추가된다. 예시3. 4개의
      송신자와 유한 버퍼를 가지는 라우터, 멀티홉 경로 경로까지 여러 라우터를 지날 때 하위 라우터에서
      오버플로우가 발생하 여 패킷로스가 일어난다면 상위 라우터까지 패킷을 전송하느라 들어간 비용이
      헛된 수고가 됨.
    </p>
    <h1>혼잡 제어에 대한 접근법</h1>
    <p>
      종단 간의 혼잡 제어 : 네트워크 계층은 트랜스포트 계층에게 혼잡제어 목적으로 아무런 도움도 주지
      않기에 TCP 프로토콜은 세그먼트 손실이 나 SmapleRTT의 증가를 네트워크 혼잡 증가로 보고 네트워크
      혼잡으 로 생각하며 윈도우의 크기를 줄인다. 네트워크 지원 혼잡 제어 : ATM안에서 ABR 혼잡 제어로
      사용되는데 XCP프로토콜은 라우터가 송신자에게 자신의 출력 링크에 제공할 수 있 는 전송률을
      알려주어 각 패킷의 출발지에서 헤더 안에 전송률 증감에 관한 정보를 넣는 것이다. 혼잡 제어에
      대한 혼잡 정보는 두 가지 방법 중 하나로 첫 번째는 송신 자에게 피드백 되는데 직접 피드백은
      라우터에서 송신자로 알림(초크 패 킷)의 형태로 보내는 직접 피드백이고 두 번째는 혼잡을 나타내기
      위해 송신자가 수신자에게 패킷 안의 특정 필드에 표시/수정 등을 하여 표시 된 패킷을 수신했을 때
      수신자가 송신자에게 혼잡 상태를 알리는 방법이 다. 후자는 RTT가 소요됨을 알아야 한다.
    </p>
    <h1>TCP 혼잡 제어</h1>
    <p>
      TCP의 혼잡 제어 메커니즘은 앞에 설명한 버퍼와 변수 외에 추가적인 혼잡 윈도우라는 변수를
      기록하는데 cwnd로 표시하며 송신자가 네트워크로 트래핏을 전송할 수 있는 비율을 제한한다. 특히
      송신 측에서는 rwnd와 cwnd를 LastByteSent – LastByteAcked를 넘지 않는다. 혼잡 상태를 알기
      위해서는 <span>손실 이벤트</span>의 발생 여부를 확인하면 되는 데 예시로 중복 ACK가 3번 왔거나
      타임아웃이 발생한 경우를 들 수 있 다. 만약 네트워크가 혼잡 상태가 아니라면 혼잡 윈도우는
      확인응답이 빠르게 오게 되면서 혼잡 윈도우는 TCP에 의해 증가하게 되는데(전송률 억제를 낮춤)
      TCP는 확인응답을 트리거 or 클록으로 사용하므로 이를
      <span>자체 클로킹(self clocking)</span>이라고 한다. 전송률을 제어하는 cwnd 값을 조정하는
      메커니즘이 주어진 경우 송신 자가 송신 속도를 정하는 방식은 다음과 같다. 1. 손실된 세그먼트
      발생 시 전송률은 줄어들어야 한다. 2. 확인응답 되지 않은 세그먼트에 대해 ACK가 도착한다면
      증가가 가능 하다. 3. 위의 방법을 반복하며 손실 이벤트가 발생하는 전송률과 아닌 전송률 사이의
      대역폭을 탐색한다.
    </p>
    <h1>혼잡 제어 알고리즘 3가지</h1>
    <p>
      슬로 스타트 : 초기 cwnd 값은 1 MSS로 초기화되고 전송률은 MSS/RTT가 된다. 확인응답을 받을
      때마다 확인응답 세그먼트 개수만큼 의 MSS를 추가시키며 지수적으로 증가시킨다. 만약 첫 번째
      타임아웃에 대한 손실 이벤트가 있으면 cwnd=1로 재설정 하고 새로운 슬로 스타트 를 시작한다.
      <span
        >또는 ssthresh(slow start threshold, 슬로 스타트 임계치 의 약자, 혼잡이 처음 발생하였을 때
        혼잡 윈도우의 반값) 값을 cwnd/2 로 정하고 선형적으로 증가시키다가 cwnd 값이 ssthresh 와
        같으면 슬 로 스타트는 종료되고 혼잡 회피 모드로 변경한다.</span
      >
      마지막으로 정해진 임의의 개수의 중복 ACK들이 검출되면 빠른 재전송을 수행하여 빠른 회 복 상태로
      들어가는 방법이다.
    </p>
    <h1>혼잡 회피</h1>
    <p>
      혼잡 회피 상태로 들어가는 시점에서 cwnd 값은 혼잡이 마지막으로 발 견된 시점에서의 값의 반으로
      되고 매 RTT마다 하나의 MSS만큼 증가 시킨다. 이 선형적 증가는 혼잡 회피 알고리즘과 같은 형태로
      동작하는 데 cwnd 값은 1로 하고 ssthresh의 값은 손실 이벤트 발생 시의 cwnd 반 값으로 한다. 만약
      하나의 손실 이벤트로 인해 3개의 중복된 ACK가 돌아오면 cwnd 값을 반으로 하고 ssthresh 값을
      중복된 ACK를 수신한 시점의 cwnd 값의 반으로 한다. 이후 빠른 회복 상태에 들어간다.
    </p>
    <h1>빠른 회복</h1>
    <p>
      cwnd 값은 잃었던 세그먼트에 대한 매 중복된 ACK를 수신할 때마다 1MSS만큼 증가하고 타임아웃
      이벤트가 발생하면 슬로 스타트와 혼잡 회피에서와의 같은 동작을 수행한 후 슬로 스타트로 초기
      설정으로 전이 한다. 빠른 회복은 필수가 아닌 권고이다. TCP Reno는 이를 채택했다.
    </p>
    <h1>TCP 혼잡 제어 복습</h1>
    <p>
      손실이 타임 아웃이 아니라 3개의 중복 ACK로 인해 발생한다면 가법적 증가(RTT마다 1MSS씩 cwnd
      선형 증가)나 승법적 감소(cwnd 절반화) 의 혼잡 제어 형식이라고 불린다. 대부분의 TCP 구현은 Reno
      알고리즘 을 이용하며 TCP 베가스 알고리즘은 좋은 처리율을 유지하면서 혼잡 회 피를 시도한다.
      기본 아이디어는 손실 발생 전 출발지와 목적지 사이의 혼잡을 발견하고 임박한 패킷 손실이
      발견되었을 때 선형적으로 비율을 낮추는 것으로 RTT를 관찰하여 임박한 패킷 손실을 예측한다. TCP
      연결의 평균 처리율은 다음과 같다
      <span>평균 처리율 = 0.75*W(손실 이벤트가 발생하는 시점의 증가 값)/RTT</span>
    </p>
    <h1>광대역 경로상의 TCP</h1>
    <p>
      TCP 연결의 평균 처리율을 구하는 다른 공식은 다음과 같다.
      <span>평균 처리율 = 1.22*MSS/RTT*루트L(L=손실률)</span>
    </p>
    <h1>TCP의 공평성</h1>
    <p>
      TCP 프로토콜을 사용하는 각 연결의 평균 전송률은 공평하게 공유하며 즉, 링크 대역폭의 분배 또한
      공평하다. 그러나 이것은 2개의 TCP 연결 만이 병목 링크를 통과하고 같은 RTT 값을 가지며 단일 TCP
      연결만을 고려하였을 때이다. 현실에서는 더 작은 RTT를 가지는 세션이 대역폭이 좀 더 빠르게
      비워지므로 혼잡 윈도우 개방이 상대적으로 빨라 RTT가 큰 TCP 연결보다 더 높은 처리율을 가진다.
      만약 TCP 기반의 애플리케이션이 다중 병렬 연결 방식을 사용한다면 링크 대역폭의 공유가 공평하게
      이루어질 수 없다.
    </p>
    <h1>명시적 혼잡 표시 – 네트워크 지원 혼잡 제어</h1>
    <p>
      TCP는 네트워크 계층으로부터는 명시적인 혼잡 표시를 받지 않고 패킷 손실 등으로부터 혼잡을
      추측할 뿐이다. 그러나 최근에는 IP와 TCP의 확장으로 명시적 혼잡 표시(Explicit Congestion
      Notification, ECN)이 제안되고 구축되었다. 네트워크 계층에서 IP 데이터그램 헤더의 서비스 형식
      필드 내에 두 비트가 ECN에 사용되는데 이 비트들의 세팅 중 하 나는 라우터가 경험하는 혼잡을
      표시한다. 이는 목적지 호스트에 알려주 고 다시 송신 호스트에게 알려주는 방식이다. 이때 목적지
      호스트는 송 신 호스트의 TCP에게 ECE(ECN Echo)비트를 세팅함으로써 혼잡 표시 를 알려준다. 송신
      호스트는 ECE 비트가 세팅된 ACK를 받게 되면 혼잡 윈도우를 반으로 줄이고 다음 전송되는 TCP
      세그먼트 헤더에 CWR(혼 잡 윈도우 축소, Congetion Window Reduced)비트를 세팅한다.
    </p>
    <h1>포워딩과 라우팅</h1>
    <p>
      포워딩: 패킷이 라우터의 입력 링크에 도달하면 그 패킷을 출력 링크로 보내야 하는데 제어 영역의
      역할이지만 예외적으로 한 기능은 데이터 영 역에서 실행된다. 라우팅: 패킷 전송 시 패킷 경로를
      설정해주는데 이를 계산하는 알고리 즘이 <span>라우팅 알고리즘</span>이다. +포워딩은 패킷을
      적절한 링크로 이동시키는 것, 라우팅은 패킷의 경로 를 결정하는 것의 차이가 있고 포워딩이 훨씬
      빠르게 수행되며 포워딩은 하드웨어에서 라우팅은 소프트웨어에서 일반적으로 실행된다. 라우터에서
      필수 불가결한 요소는 <span>포워딩 테이블</span>이다. 라우터는 도착하 는 패킷의 헤더 필드 값을
      조사하여 패킷을 포워딩(전달)하는데 이 값을 라우터의 포워딩 테이블의 내부 색인으로 사용한다.
      쉽게 말해 헤더가 가지고 있는 값에 따라 라우터가 라우팅 알고리즘으로 짜인 포워딩 테이 블을
      참고하여 출력 링크를 결정한다.
    </p>
    <h1>제어 영역의 전통적 접근</h1>
    <p>
      라우터는 포워딩과 라우팅 기능 모두 가지고 있어야 하며 다른 라우터의 라우팅 알고리즘과 소통하며
      포워딩 테이블의 값들을 계산한다. 이는 라 우팅 프로토콜에 따라 라우팅 메시지를 교환하며
      이루어진다.
    </p>
    <h1>제어 영역의 SDN 접근</h1>
    <p>
      라우터에 있는 포워딩 테이블을 원격으로 계산과 분배를 해주며 라우팅 기기는 포워딩만 수행한다.
      원격 컨트롤러가 높은 신뢰성과 중복성을 갖 춘 원격 데이터 센터에 설치 가능하며 ISP나 제 3자가
      관리도 가능하다. 즉 각 라우터가 아닌 원격 컨트롤러가 라우터들과 라우팅 정보와 포워딩 테이블을
      포함한 메시지를 교환하는 방식으로 소통한다. 이러한 접근이
      <span>SDN(Software Defined Networking)</span>이다.
    </p>
    <h1>네트워크 서비스 모델</h1>
    <p>
      네트워크 서비스 모델은 송수신 호스트 간 패킷 전송 특성을 정의한다. -보장된 전달: 패킷이 소스
      호스트에서부터 목적지 호스트까지 도착하는 것을 보장한다. -지연 제한 이내의 보장된 전달: 패킷
      전달 + 호스트 간의 특정 지연 제 한 안에 전달한다. 순서화 패킷 전달: 순서대로 전달을 보장한다.
      -최소 대역폭 보장: 송신 호스트가 특정한 비트 속도 즉, 할당된 대역폭 만큼 사용한다면 전송을
      보장한다. -보안 서비스: 모든 데이터그램을 송신 측에서 암호화, 수신 측에서 해 독이 가능하도록
      트랜스포트 계층의 모든 세그먼트의 기밀성을 유지해준 다. +최선형 서비스 : 위에 내용과 같음,
      이보다 좋은 서비스 모델이 있는데 ATM 네트워크 구조의 경우 순서화 패킷 전달 서비스, 지연 제한
      이내 의 보장된 전달 서비스, 최소 대역폭을 보장한다. +링크 계층 스위치와 라우터의 차이점: 링크
      계층 프레임의 필드 값에 근거하여 포워딩을 결정하거나 네트워크 계층 필드 값으로 포워딩을 결
      정하느냐에 따라 구분된다.
    </p>
    <h1>라우터의 내부</h1>
    <p>
      4가지 요소로 다음과 같다. -입력포트 : 라우터로 들어오는 입력 링크로 물리 계층 기능을 수행하며
      들어오는 링크의 반대편에 있는 링크 계층과 상호 운용을 위해 링크 계 층 기능도 수행한다. 또한
      입력 포트에서 검색 기능을 수행하는데 포워 딩 테이블을 참조하여 도착한 패킷이 스위칭 구조를
      통해 라우터 출력 포트를 결정한다. 또한 라우팅 프로토콜 정보를 전달하는 패킷은 입력 포트에서
      라우팅 프로세서로 전달된다. -스위칭 구조 : 라우터의 입력 포트와 출력 포트를 연결하는데 라우터
      내부에 포함되어 있다. -출력 포트 : 출력 포트는 스위칭 구조에서 수신한 패킷을 저장하고 필 요한
      링크 계층 및 물리적 계층 기능을 수행하여 출력 링크로 패킷을 전 송한다. 링크가 양방향이면
      연결된 링크의 입력 포트와 한 쌍이다. -라우팅 프로세서 : 라우팅 프로세서는 제어 영역 기능을
      수행하며 전통 적인 라우터에서는 라우팅 프로토콜을 실행하고 라우팅 테이블과 연결된 링크 상태
      정보를 유지 관리하며 라우터의 포워딩 테이블을 계산한다. 반면에 SDN 라우터에서 라우팅
      프로세서는 <span>원격 컨트롤러</span>와 통신하며 계산된 포워딩 테이블을 수신하고 입력 포트에
      설치한다. +입력포트, 출력포트, 스위칭 구조는 거의 하드웨어로 구현되는데 이는 데이터 영역의
      특성이며 나노초 단위로 작동한다. 반면에 제어 영역인 라우팅 프로세서는 라우팅 프로토콜 실행 및
      원격 컨트롤러와 통신 및 관리 기능 수행 등을 하며 밀리초 ~ 2초 단위로 작동한다.
    </p>
    <h1>입력 포트 처리 및 목적지 기반 전송</h1>
    <p>
      입력 포트에서 수행되는 검색은 라우터 동작의 핵심인데 라우터는 포워 딩 테이블을 사용하여 도착
      패킷이 스위칭 구조를 통해 전달되는 출력 포트를 검색한다. 포워딩 테이블은 라우팅 프로세서에서
      <span>입력 회선 카드</span> 로 복사되는데 이때 섀도 복사본을 사용하면 패킷 단위로 중앙 집중식
      라우팅 프로세서를 호출하지 않아도 되므로 병목 현상을 피할 수 있다. 입력 패킷을 스위칭할 출력
      포트가 각 패킷의 대상 주소를 기반으로 하 는 것은 불가능하므로 목적지 주소의 범위에 따라
      엔트리를 구성한 포워 딩 테이블을 생성해야 한다. 이런 형식의 포워딩 테이블에서 라우터는 패킷의
      목적지 주소의 프리픽스(IP주소 비트 앞부분)를 테이블의 엔트리 와 대응시키고 존재한다면 연관된
      링크로 보낸다. 만약 대응하지 않는다 면 <span>최장 프리픽스 대응 규칙</span>을 적용한다. 이는
      가장 유사도가 높은 엔트 리를 찾고 연관된 링크로 보내는 것이다. 검색을 통해 패킷의 출력 포트가
      결정되면 패킷을 스위칭 구조로 보낼 수 있는데 일부 설계에서 다른 입력 포트가 현재 스위칭 구조를
      사용하 고 있다면 일시적으로 차단하고 입력 포트에 대기한 후 나중에 들어가도 록 예약된다.
    </p>
    <h1>스위칭</h1>
    <p>
      스위칭 구조는 패킷이 입력 포트에서 출력 포트로 실제로 스위칭 되는 구조를 통과하므로 라우터의
      핵심이라고 할 수 있는데 3가지 방법이 가 능하다. -메모리를 통한 교환 : 가장 단순하며 초기의
      라우터는 라우팅 프로세서 를 직접 제어하여 패킷을 스위칭하는 방법으로 전통적인 운영체제의 I/O
      장치처럼 작동한다. 패킷이 도착하면 입력 포트는 라우팅 프로세서 에게 인터럽트를 보내고 패킷을
      프로세서 메모리에 복사한 후 포워딩 테 이블에서 적절한 출력 포트를 찾아 출력 버퍼에 복사한다.
      이 방법은 두 패킷을 동시에 전달할 수 없다. 최근 라우터는 메모리를 통해 스위칭하 는데 초기와
      차이점은 프로세서를 통하지 않고 입력 회선 카드가 작업을 대신 수행한다. -버스를 통한 교환 :
      입력 포트는 라우팅 프로세서의 개입 없이 공유 버 스를 통해서 직접 출력 포트로 패킷을 전송하는데
      일반적으로 미리 준비 된 입력 포트 스위치 내부 라벨이 로컬 출력 포트를 나타내는 패킷에게
      전송되거나 버스에 패킷을 전송하여 수행된다. 라벨은 스위치 내에서 버 스를 통과하기 위해서만
      사용하므로 출력 포트에서 제거되며 한 번에 하 나의 패킷만 버스를 통과할 수 있으므로 나머지는
      대기한다. 이 때문에 버스의 속도에 의해 라우터의 교환 속도가 제한된다. 그러나 작은 지역 의
      네트워크 라우터에서는 충분하다. -인터커넥션 네트워크를 통한 교환(크로스바) : 크로스바 스위치는
      N개 의 입력 포트와 n개의 출력 포트로 이루어진 N+n 버스로 구성된 인터 커넥션 네트워크이며 수직
      버스는 교차점에서 수평 버스와 교차하며 스 위치 구조 컨트롤러에 의해 언제든지 여닫을 수 있다.
      이는 다른 입출력 버스를 사용하여 동시에 전달할 수 있지만 출력 포트가 같다면 하나의 패킷은
      대기해야 하는 특성이 있다. +<span>시스코CRS</span>는 3단계 논 블록킹 스위칭 전략을 사용함.
    </p>
    <h1>출력 포트 프로세싱</h1>
    <p>
      출력 포트의 메모리에 저장된 패킷을 가져와서 출력 링크를 통해 전송하 며 이를 위해 패킷 선택 및
      대기열 제거 링크 계층 및 물리 계층 전송 기능을 수행한다.
    </p>
    <h1>입력 큐잉</h1>
    <p>
      지연 없이 도착하는 모든 패킷을 전송하기에 스위치 구조가 빠르지 않으 면 출력 포트로 전송되기
      위해 대기하는 상황이 발생한다. FCFS, 크로 스바 방식이라고 가정했을 때 출력 포트가 같은 두 개의
      입력 포트에 위 치한 패킷이 있다면 하나의 패킷은 대기하는데 이 현상을 입력 대기 중 인
      스위치에서의 <span>HOL(Head of the Line)차단</span>이라고 한다. 입력 링크에 서 패킷 도착
      속도가 용량의 58%가 되면 HOL차단으로 인해 입력 큐가 무한정 길이로 증가한다. 즉, 패킷로스가
      발생한다.
    </p>
    <h1>출력 큐잉</h1>
    <p>
      출력 포트에서는 출력 링크에 단일 패킷만 보낼 수 있으므로 큐잉이 발 생한다. 출력 포트에
      들어오는 패킷을 저장할 메모리가 충분하지 않을 때 새로 도착한 패킷이나 대기 중인 패킷을 제거해
      공간을 확보해야 한 다. 또는 버퍼가 가득 차기 전에 패킷을 삭제시켜 송신자에게 혼잡 신호 를
      제공하는 것이 옳은 방법일 수 있다. 이러한 패킷 삭제와 패킷 마킹 정책들을
      <span>AQM(active queue management)알고리즘</span> 이라고 한다. 패킷이 동시에 도착하여 같은
      출력 포트를 향할 때 어떠한 패킷을 우선 보낼 것인지는 <span>패킷 스케줄러</span>가 담당한다.
    </p>
    <h1>패킷 스케줄링</h1>
    <p>
      운영체제의 cpu 스케줄링과 유사한 개념으로 이루어져 있다. -FIFO(선입선출) : 출력 링크 큐에
      도착한 순서대로 전송할 패킷을 선택 한다. -우선순위 큐잉 : 큐에 도착하면 우선순위 클래스로
      분류되고 네트워크 오퍼레이터는 네트워크 관리 정보(라우팅 프로토콜 메시지 등)를 운반하 는
      패킷이 사용자 트래픽 보다 우선순위를 수신하도록 구성하기도 한다. 또한 실시간 VoIP 패킷이 SMTP
      등과 같은 전자메일 패킷보다 우선순위 를 받을 수도 있다. 동일한 우선순위를 가진 패킷들은 FIFO
      정책을 따 른다. + 비 선점 우선순위 큐잉은 전송이 시작되면 우선순위가 높은 패 킷이 들어오더라도
      진행 중인 작업을 멈추지는 않는다. -라운드로빈 큐잉 : cpu의 라운드로빈 스케줄링과 유사하지만
      클래스 개념으로 도착하는 패킷들을 구분해놓고 클래스1의 패킷이 전송되었다면 다음 클래스의
      패킷을 전송하고 다시 클래스1의 패킷으로 돌아와서 전송 하는 방식이며 해당 순번인 클래스가
      비어있다면 남은 패킷을 전송한다. 라운드로빈 큐잉은 일반적인 WFQ 규칙과 유사한데 교재의
      예시처럼 순 환 방식으로 작동한다. 그러나 WFQ는 라운드로빈과 다르게 클래스마다 부여되는 시간이
      다르다. 이는 최악의 경우에도 특정 클래스가 일정한 대역폭만큼을 사용할 수 있도록 보장하기 위한
      것인데 이는 이상적인 상 황이며 패킷이 전송될 시 다른 패킷에 방해되지 않기 때문에 현실과는
      거리가 멀다.
    </p>
    <h1>TCP Reno</h1>
    <p>
      -슬로스타트로 시작 임계점에 다다르면 1씩 증가 -중복 ACK 발생 시 ssthresh(임계치) 와 cwnd
      반으로 줄임 -타임아웃 이벤트 발생하면 임계점은 유지, cwnd만 1로 설정, 슬로스타트 다시 시작
    </p>
    <h1>TCP Tahoe</h1>
    <p>
      -슬로스타트 시작, 임계점에서는 1씩 증가 -중복 ACK나 타임아웃 이벤트 발생 시 임계치 절반 cwnd는
      무조건 1로 설정 후 다시 슬로 스타트
    </p>
    <h1>TCP Cubic</h1>
    <p>
      -슬로스타트 시작, 혼잡 발생 시 30프로 감소 -Wmax 는 혼잡 발생 시 윈도우 값이며 혼잡 발생
      안하면 존나 빠르게 다시 증가해서 혼잡 윈도우 값을 찾음 -임계치 다다르기 전 증가율 감소,
      수렴하는 느낌 -혼잡 상태 기반이 아니라 사전 예방하는 듯
    </p>
  </body>
</html>
`
